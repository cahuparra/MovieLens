---
title: "Carlos_Parra MovieLens Project"
author: "Carlos Parra"
date: "10/21/2021"
output: pdf_document
---

```{r setup, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
tinytex::install_tinytex()
# to uninstall TinyTeX, run
# tinytex::uninstall_tinytex()
```

# Movie Ratings Prediction
### Carlos Parra
### November. 2021
## Introduction
### November 2021
This document includes the information (data and algorithm) required to predict  movie ratings using machine learning models described in the section *Movie Prediction Algorithm*

## Libraries
Next libraries were be used to generate the rating
* library(tidyverse)
* library(caret)
* library(data.table)
* library(lubridate)


## Data
The dataset edx will be splitted in train_set and test_set to develop and test the algorithm; the validation dataset will be used to evaluate the RMSE of the final algorithm.  edx and validation datasets are generated using the e code provided in the _Capstone section_ using Movielens 10M dataset.
``` {r data_generation,  message=FALSE, warning=FALSE}
# Note: this process could take a couple of minutes

if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(lubridate)) install.packages("lubridate", repos = "http://cran.us.r-project.org")

library(tidyverse)
library(caret)
library(data.table)
library(lubridate)

# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip

dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")

# if using R 3.6 or earlier:
#movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(levels(movieId))[movieId],
#                                           title = as.character(title),
#                                           genres = as.character(genres))
# if using R 4.0 or later:
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(movieId),
                                           title = as.character(title),
                                           genres = as.character(genres))


movielens <- left_join(ratings, movies, by = "movieId")

# Validation set will be 10% of MovieLens data
set.seed(1, sample.kind="Rounding")


test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in validation set are also in edx set
validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set
removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)
```

Split edx dataset into separate training and test sets.
``` {r split_edx,  message=FALSE, warning=FALSE}

# split edx in test and train datasets
test_index <- createDataPartition(edx$rating, times = 1, p = 0.5, list = FALSE)
train_set <- edx %>% slice(-test_index)
test_set <- edx %>% slice(test_index)
```` 


## Predictors
Predictors were selected based on next criteria:
. Avoid predictors that are highly correlated with others
. Predictors have very few on-unique values
. Predictors that have close to zero variation.
The chosen predictors are:
1. movieId
2. userId
3. frequency rating peruser 
According to Koren (2) the number of ratings a user gave on a specific time explains a portion of the variability of the data during that time. The Frequency Fui is the overall number of ratings that user u gave on week tui. 
4. week. Variable generated using TimeStamp. The trend line shows that the rates decline over time
``` {r week_variable,  message=FALSE, warning=FALSE}
#generate predictor day
train_set<- train_set %>% 
  mutate(day = round_date(as_datetime(timestamp), unit = "day"))
```` 
The number of unique userIds (n_users), movieIds (n_movies), and weeks (n_weeks) are:
``` {r size_predictors,  message=FALSE, warning=FALSE}
# Identify unique number of predictors (movie, user, time (weeks))
train_set %>% 
  summarize(n_users = n_distinct(userId),
              n_movies = n_distinct(movieId),
            n_dayss = n_distinct(day))
```` 
The interactions between users, movies and weeks produce different rating values due to the effect associated with the movie, user or week independently.
Next graphics represent the dispersion of the average rate (avg_rate) versus the predictors.
``` {r predictor_avgrate,  message=FALSE, warning=FALSE}
# grahpich avg_rate versus predictors
train_set %>% 
  group_by(movieId) %>% 
  summarize(avg_rate=mean(rating)) %>%
  arrange(desc(avg_rate)) %>%
  ggplot(aes(movieId,avg_rate)) + 
  geom_point()
train_set %>% 
  group_by(userId) %>% 
  summarize(avg_rate=mean(rating)) %>%
  arrange(desc(avg_rate)) %>%
  ggplot(aes(userId,avg_rate)) + 
  geom_point()
#Calculate User rating fequency per day
f_ut<-train_set %>% 
  group_by(userId) %>%
  group_by(day) %>%
  summarize(f_ut=abs(log(n(),base=10000)))
f_ut %>%
  ggplot(aes(day,f_ut)) + 
  geom_point() 
train_set %>% 
  group_by(day) %>% 
  summarize(avg_rate=mean(rating)) %>%
  arrange(desc(avg_rate)) %>%
  ggplot(aes(day,avg_rate)) +
    geom_point()
```` 

## Movie Prediction Algoritm
In order to optimize the movie prediction a combination of Machine Learning models will be used  to optimize the result (minimize **root mean squared error- RMSE** between the predicted rating and the actual rating)
``` {r rmse_function, message=FALSE, warning=FALSE}
#function that computes the Residual Means Squared Error for a vector of ratings and their corresponding predictors
RMSE <- function(true_ratings, predicted_ratings){                                         
  sqrt(mean((true_ratings - predicted_ratings)^2))
}
```

 
1. **Regresion Models**
RMSE progressive results 

1.1 Naive Method: assume the **same rating for all movies and all users**. 
Estimate Mu as the average rating for all movies and users
    $Y_{u,i} = \mu + \espilon_{u,i}$
Epsilon represents independent errors sampled for the same distribution centered at zero; it explains the remaining variability
Predict the same rating for all movies using an average rating (Mu) for all movies and users. 
``` {r estimate_Mu,  message=FALSE, warning=FALSE}
# Estimate RMSE using Mu (average rating of all movies across all users)
mu<-mean(train_set$rating)
predicted_ratings<-mu
test_set <- test_set %>% 
     semi_join(train_set, by = "movieId") %>%
     semi_join(train_set, by = "userId")
rmse1<-RMSE(test_set$rating,predicted_ratings)
cat("Baseline/average rating across all movies and users (mu):",mu)
difference<-0
rmse_results <- tibble(method = "Baseline/average:", RMSE = rmse1, Difference = difference) 
rmse_results %>% knitr::kable() 

```

1.2. Add the **Movie effect (b_i)**. 
b_i is estimated as the average rating for movie i.
   $Y_{u,i} = \mu + b_{i} + \espilon_{u,i}$
Calculate b_i average rating per movie i.
    $b_{i} = Y_{u_i} - \mu$
``` {r estimate_bi,  message=FALSE, warning=FALSE}
# Estimate RMSE using b_i and mu
b_i <- train_set %>% 
     group_by(movieId) %>% 
     summarize(b_i = mean(rating - mu)) 
cat("Histogram average movie rating (b_i)")
b_i %>% qplot(b_i, geom = "histogram", bins = 10, data = ., color = I("black"))

predicted_ratings<-test_set %>%
  left_join(b_i, by = "movieId") %>%
  mutate(pred =  b_i + mu) %>%
    .$pred
  
rmse2<-RMSE(test_set$rating,predicted_ratings)
difference<-rmse2-rmse1
rmse_results <-bind_rows(rmse_results,tibble(method = "Movie Effect Model", RMSE = rmse2, Difference = difference) )
rmse_results %>% knitr::kable()  

```

1.3 Add the **user specific effect (b_u)**. 
b_u is estimated by taking the average of the residuals obtained after removing the overall mean and the movie effect from the rating y_ui 
    $Y_{u,i} = \mu + b_{i} + b_{u} + \espilon_{u,i}$
Calculate the user specific effect b_u per user u
    $b_{u} = Y_{u_i} - \mu - b_{i}$
``` {r estimate_bu,  message=FALSE, warning=FALSE}
# Estimate RMSE using b_i and b_u
b_u <- train_set %>% 
     left_join(b_i, by='movieId') %>%
     group_by(userId) %>%
     summarize(b_u = mean(rating - mu - b_i))

cat("Histogram average user rating (b_u)")
b_u %>% qplot(b_u, geom = "histogram", bins = 10, data = ., color = I("black"))

predicted_ratings<-test_set %>%
    left_join(b_i, by='movieId') %>%
    left_join(b_u, by='userId') %>%
    mutate(pred = mu + b_i + b_u) %>%
    .$pred
rmse3<-RMSE(test_set$rating,predicted_ratings)
difference<-rmse3-rmse2
rmse_results <-bind_rows(rmse_results,tibble(method = "Movie and User Effect Model", RMSE = rmse3, Difference = difference) )
rmse_results %>% knitr::kable()    
```  

1.4 Include the **frequency effect**
The frequency is the number of ratings a user gave on a specific time denoted as $F_{ut}$. I will use a Koren's (2) rounded logarithm of $F_{ut}, denoted as:
    $f_{ut} = \left\loga_{a} F_ui\right$
$f_{ut}$ is driven by user u, and it influences the item-biases:
    $Y_{u,i} = \mu + b_{i} - b_{i}*f_{ut} + \espilon_{u,i}$
    $b_{i} = (Y_{u_i} - \mu)/(1-f_{ut})$
Notes on $f_{ut}$
. The formula was fined tuned to optimize RMSE: 
    The chosen base for the logarithm is 10000 to optimize the RMSE.
    The chosen denominator of $f_{ut} to estimated $b_{i} was 1000
``` {r estimate_fut,  message=FALSE, warning=FALSE}
# Estimate RMSE using b_i andjusted by f_ut. and b_u
b_i <- train_set %>%
  left_join(f_ut,by="day") %>%
  group_by(movieId) %>% 
  summarize(b_i = mean(rating - mu)/(1 - mean(f_ut )/1000)) 
b_u <- train_set %>% 
  left_join(b_i, by='movieId') %>%
  group_by(userId) %>%
  summarize(b_u = mean(rating - mu - b_i))
predicted_ratings<-test_set %>%
  left_join(b_i, by='movieId') %>%
  left_join(b_u, by='userId') %>%
  mutate(pred = mu + b_i + b_u) %>%
  .$pred
rmse4<-RMSE(test_set$rating,predicted_ratings)
difference<-rmse4-rmse3
rmse_results <-bind_rows(rmse_results,tibble(method = "Movie adjusted by frequency and  User Model", RMSE = rmse4, Difference = difference) )
rmse_results %>% knitr::kable()    
``` 
I will discard this method because the improvement was negligible; there could be other potential formulas to combine frequency with b_i that produce relevant reductions as indicated by Koren. 

1.5 Add the **time effect**. 
The graphic in the section Predictors shows that ratings reduce over time, I introduce the variable b_t to capture this effect
    $Y_{u,i} = \mu + b_{i} + b_{u} + b_{t} + \espilon_{u,i}$
Calculate the user specific effect b_u per user u
    $b_{t} = Y_{u_i} - \mu - b_{i} - b_{u}$
Calculate the time effect b_t per day and 
``` {r estimate_bt,  message=FALSE, warning=FALSE}
# Estimate RMSE using b_t, b_u, b_i and mu
b_i <- train_set %>% 
     group_by(movieId) %>% 
     summarize(b_i = mean(rating - mu)) 
b_u <- train_set %>% 
  left_join(b_i, by="movieId") %>%
  group_by(userId) %>%
  summarize(b_u = mean(rating - b_i - mu))
# add day variable to tes_set to generate b_t
test_set<- test_set %>% 
  mutate(day = round_date(as_datetime(timestamp), unit = "day"))
b_t <- train_set %>%
  left_join(b_i, by='movieId') %>%
  left_join(b_u, by='userId') %>%
  group_by(day) %>%
  summarize(b_t = mean(rating - mu - b_i - b_u))

predicted_ratings <-  test_set %>% 
  left_join(b_i, by = "movieId") %>%
  left_join(b_u, by = "userId") %>%
  left_join(b_t, by = "day") %>%
  mutate(pred = ifelse(is.na(mu + b_i + b_u + b_t),mu + b_i +b_u,mu + b_i + b_u + b_t   )) %>%
  .$pred
rmse5 <- RMSE(predicted_ratings, test_set$rating)
difference<-rmse5-rmse3
rmse_results <-bind_rows(rmse_results,tibble(method = "Movie,User, and Time Model", RMSE = rmse5, Difference = difference) )
rmse_results %>% knitr::kable()  
```


2. **Regularization** -  penalize large estimates that come from small sample sizes 
The algorithms described in prior section give the same weight to any estimate regardless of the size of the sample. The total variability generated by the effect of the size  can be inhibited by penalizing large estimates that come from sample samples, similar to th Bayesian approach to shrunk predictions. A penalty term is added to the equation:
    $\frac(1)(n)\sum_(u,i)\left(y_{u,i} - \mu - b_{i} - b_{u} - b_{t}\right)^(2) = \lambda \left( \sum_{i} b_{i}^(2) + \sum_{u} b_{u}^(2) + \sum_{t} b_{t}^(2) \right)$
Next routine will identify the optimal lambda in the range 0, 10 with 0.25 increments

``` {r estimate_lambda,  message=FALSE, warning=FALSE}
lambdas <- seq(0,10, 0.25)
rmses <- sapply(lambdas, function(l){
  b_i <- train_set %>%
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mu)/(n()+l))
 
  b_u <- train_set %>% 
    left_join(b_i, by="movieId") %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - b_i - mu)/(n()+l))
  
  b_t <- train_set %>%
    left_join(b_i, by='movieId') %>%
    left_join(b_u, by='userId') %>%
    group_by(day) %>%
    summarize(b_t = sum(rating - mu - b_i - b_u)/(n()+l))
  predicted_ratings <- 
    test_set %>% 
    left_join(b_i, by = "movieId") %>%
    left_join(b_u, by = "userId") %>%
    left_join(b_t, by = "day") %>%
    mutate(pred = ifelse(is.na(mu + b_i + b_u + b_t),mu + b_i +b_u,mu + b_i + b_u + b_t   )) %>%
    .$pred
  return(RMSE(predicted_ratings, test_set$rating))
})
qplot(lambdas, rmses)  
lambda <- lambdas[which.min(rmses)]
rmse6<- min(rmses)
difference<-rmse6-rmse5
rmse_results <-bind_rows(rmse_results,tibble(method = "Regularized Movie, User, and and Time Model", RMSE = rmse6, Difference = difference) )
rmse_results %>% knitr::kable() 

```

.
bhat_i(lambda)=(1/(lambda+ni))*sum[u-1,n_i](Y_u,i-Muhat)
where n_i is the number of ratings made for movie i.
When n_i is very large, which will give us a stable estimate, then lambda is effectively ignored because n_i plus lambda is about equal to n_i.
However, when n_i is small, then the estimate of b_i is shrunken towards zero. 
lambda is a tuning parameter, cross-validation will be used to select the number that minimizes RMSE.

4. Item-Item approach
5.  User-user approach

???(square root of the) number of days since her first rating

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r cars}
# summary(cars)
```

## Including Plots

You can also embed plots, for example:
Histogram movie ratings (count)
Histogram user rating (count)
Histogram ratings through

```{r pressure, echo=FALSE}
# plot(pressure)
```



## References
1. Irizarry, Rafael A (2021-07-03). Introduction to Data Science Data Analysis and Prediction Algorithms with R
2. Koren, Yehuda (August 2009). The Bellkor Solution to the Netflix Grand Prize
3. Chen, Edwin. Winning the Netflix Prize: A Summary
4. Wickham, Hadley and Grolemund, Garrett. R for Data Science Visualize, Model, Transform, Tidy and Import Data
